#4.1 
import numpy as np
from sklearn import preprocessing 

#Create feature
feature = np.array([	[-500.5],
			[-100.1],
			[0],
			[100.1],
			[900.9]])

#create scaler ??


#Scale feature

scaled_feature = minmax_scale.fit_transform(feature)

#show feature

scaled_feature

array([[0.], 
	[0.28571429]
	[0.35714286]
	[0.42857143]
	[1.        ]])


#4.2

#load libaries
import numpy as np
from sklearn import preprocessing

#create feature

x = np.array([	[-100.1],
		[-200.2],
		[500.5],
		[600.6],
		[9000.9]])

#create scaler
scaler = preprossing.StandardScaler()

#Transform the feature
standardlized = scaler.fit_transform(X)

#show feature
standardized

array([[-0.76058268],
	[-0.54177196],
	[-0.35009716],
	[-0.32271504],
	[1.97516685]])

#Discussion

#Print mean and standard deviation
print("Mean:", round(standardized.mean()))
print("Standartd deviation", standardized.std())

Mean: 0.0
Standard deviation: 1.0


#Create scaler
robust_scaler = preprocessing.RobustScaler()

robust_scaler.fit_transform(x)

array([[-1.87387612 ],
	[-0.875     ],
	[0.         ],
	[0.125      ],
	[10.61488511]])


#4.3 
#Transform feature matix
features_l1_norm = Normalizer(norm="l1").transform(features)

#Show feature matrix
features_l1_norm

array([[0.5	,  	 0.5  ], 
	[0.2444444, 0.75555556],
	[0.06912442, 0.93087558],
	[0.04524008, 0.95475992],
	[0.76760563, 0.23239437]])

#print sum
print("Sum of the first observation\'s values:", 
features_l1_norm[0,0] + features_l1_norm[0,1])

Sum of the first observation's values: 1.0

#4.4

#load libraries 
import numpy as np
form sklearn.preprocessing import PolynomialFeatures

#Create feature matrix

???? arry = mp.array([[2,3],[2,3]])

	
#create PolynomialFeatures object
polynomial_interaction = PolynomialFeatures(degree=2, include_bais=False)

#Create Polynomial Features object
polynomial_interaction.fit_transform(features)

array([[2., 3., 4., 6., 9.,], 
	[2., 3., 4., 6., 9.,],
	[2., 3., 4., 6., 9.,]])
 
interaction = PolynomialFeatures(degree=2,
			interaction_only=True, include_bias=False)
			interaction.fit_transform(features)

array([[2., 3., 6.],
	[2., 3., 6.],
	[2., 3., 6.]])

#Appy function
df.apply(add_ten)
	
#4.5

#load libaries
import numpy as np
form sklearn.preprocessing import FunctionTransformer

#Create feature matrix
features = np.arry([[12,13],
		[12,13],
		[12,13]])

#load library
import pandas as pd

#Create DataFrame
???


4.6
#loard libraries
import numpy as np 
from sklearn.covarlance import EllipticEnvelope
from sklearn.datasets import make_blobs

#Create simultaed data
features, _ = make_blobs(n_samples = 10, 
		n_features = 2, 
		centers = 1, 
		random_state = 1)

#Replace the first  observation's values with extrem values
features[0,0] = 1000
features[0,1] = 1000

#Create detector

outlier_detector = EllipticEnvelope(contamination=.1)

#Fit detector
outlier_detector.fit(features)

#Predict outlies
outlier_detector.predict(features)

array([-1, 1, 1, 1, 1, 1, 1, 1, 1, 1])

#Create one feature
feature = features [:,0]

#Create a function  to return index of outliers

def indicies_of_outliers(x):
q1, q3 = np.prcentile(x, [25,75])
iqr = q3 - q1
lower_bound = q1 - (iqr * 1.5)
upper_bound = q3 + (iqr * 1.5)
return np.where((x>upper_bound) | (x<lower_bound))

#Run function
indicies_of_outliets(feature)

(array[0]),)
 
#4.7 Handling Outliers

import pandas as pd

#Dataframe
houses = pd.Dataframe()
houses['Price'] = [534433, 392333, 293222, 4322032]
houses['Bathrooms'] = [2, 3.5, 2, 166,]
houses['Square_Feet'] = [1500, 2500, 1500, 48000]

#Filter observations
house[house['Bathrooms']<20]

#load library
import numpy as np

#create feature based on boolean conditions
houses["Outlier"]= np.where(houses["Bathrooms"] < 20, 0, 1)
#Show data

houses
# log feature
houses["Log_Of_Square_Feet"] =[np.log(x) for x in houses["Square_Feet"]]

#show houses
houses

#Discretizating Features

import numpy as np
from sklearn.preprocessing import Binarizer
#create feature

age = np.array([[6], 
		[12],
		[20], 
		[36], 
		[65]]) 
#create binarizer
binarizer = Binarizer(18)

#Transform feature
binarizer.fit_transform(age)

array([[0],
	[0],
	[1],
	[1],
	[1]])

#Bin feature
np.digitize(age, bins=[20,30,64])

array([[0],
	[0],
	[1],
	[2],
	[3]])

#Bin feature
np.digitize(age, bins=[20,30,64], right=True)

array([[0],
	[0],
	[1],
	[2],
	[3]])

#Discussion

#Bin feature
np.digitize(age, bins=[18])

array([[0],
	[0],
	[1],
	[1],
	[1]])

#4.9 Grouping Observations Using Clustering

#Solution

import pandas as pd
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans

#Make simulated feature matrix

features, _ = make_blobs(n_samples = 50, 
			n_features = 2, 
			centers = 3, 
			random_state = 1)

#Create DataFrame
dataframe = pd.DataFrame(features, columns=["feature_1", "feature_2"])

#Make k-means clusterer

clusterer = KMeans(3, random_state=0)

#Fit clusterer

clusterer.fit(features)

#Predict values
dataframe["group"] = clusterer.predict(features)

#View first few observations

dataframe.head(5)

#4.10 Deleting Observations with Missing Values

import numpy as np

#create feature matrix

features = np.array([[1.1, 11.1], 
			[2.2, 22.2], 
			[3.3, 33.3],
			[4.4, 44.4]
			[np.nan, 55]])

#Keep only observations that are not (denoted by-) missing features
[-np.isnan(features).any(axis=1)]

array([[1.1, 11.1], 
	[2.2, 22.2], 
	[3.3, 33.3],
	[4.4, 44.4]])

#alternative
import pandas as pd

#load data

dataframe = pd.Dataframe(features, columns=["feature_1", "feature_2"])

#Remove observation with missing values

dataframe.dropna()

#4.11 Imputing Missing Values

#load libraries

import numpy as np
from fancyimpute import KNN
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_blobs

#make a simulated feature matrix

features,_ = make_blobs(m_samples = 1000,
				n_features =2, 
				random_state = 1)

#standardlize the features

scaler = StandardScaler()
standardlized_features = scaler.fit_transform(features)

#Replace th fist feature's first value with  a missing value

true_value = standardlized_feature[0,0]
standardlized_feature[0,0] = np.nan

#predict th missing value in the feature matrix

features_knn_imputed = KNN(k=5, verbose=0).complete(standardlized_features)

#Compare true imputed values

print("True Values", true_value)
print("Imputed Value:", features_knn_imputed[0,0])

#Alternatively, we can use scikit-learn's Imputer...

#load library

from sklearn.preprocessing import Imputer

#Create Imputer

mean_imputer = Imputer(strategy="mean", axis=0)

features_mean_imputed =mean_imputer.fit_transform(features)

#Compare true and imputed values

print("True Values", true_value)
print("Imputed Value:", features_mean_imputed[0,0])

